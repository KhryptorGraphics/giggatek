# GigGatek Alert Rules
# Comprehensive alert definitions for infrastructure, services, and business processes

groups:
# System resource alerts
- name: system_alerts
  rules:
  # High CPU usage
  - alert: HighCPUUsage
    expr: instance:node_cpu_utilization:percent > 80
    for: 5m
    labels:
      severity: warning
      category: system
    annotations:
      summary: "High CPU usage on {{ $labels.instance }}"
      description: "CPU usage is {{ $value | printf \"%.2f\" }}% for the last 5 minutes on {{ $labels.instance }}"
      runbook: "docs/monitoring/RUNBOOKS.md#high-cpu-usage"
      dashboard: "https://grafana.giggatek.com/d/system-overview"
  
  # Critical CPU usage
  - alert: CriticalCPUUsage
    expr: instance:node_cpu_utilization:percent > 95
    for: 5m
    labels:
      severity: critical
      category: system
    annotations:
      summary: "Critical CPU usage on {{ $labels.instance }}"
      description: "CPU usage is {{ $value | printf \"%.2f\" }}% for the last 5 minutes on {{ $labels.instance }}"
      runbook: "docs/monitoring/RUNBOOKS.md#critical-cpu-usage"
      dashboard: "https://grafana.giggatek.com/d/system-overview"
  
  # High memory usage
  - alert: HighMemoryUsage
    expr: instance:node_memory_utilization:percent > 80
    for: 5m
    labels:
      severity: warning
      category: system
    annotations:
      summary: "High memory usage on {{ $labels.instance }}"
      description: "Memory usage is {{ $value | printf \"%.2f\" }}% for the last 5 minutes on {{ $labels.instance }}"
      runbook: "docs/monitoring/RUNBOOKS.md#high-memory-usage"
      dashboard: "https://grafana.giggatek.com/d/system-overview"
  
  # Critical memory usage
  - alert: CriticalMemoryUsage
    expr: instance:node_memory_utilization:percent > 95
    for: 2m
    labels:
      severity: critical
      category: system
    annotations:
      summary: "Critical memory usage on {{ $labels.instance }}"
      description: "Memory usage is {{ $value | printf \"%.2f\" }}% for the last 2 minutes on {{ $labels.instance }}"
      runbook: "docs/monitoring/RUNBOOKS.md#critical-memory-usage"
      dashboard: "https://grafana.giggatek.com/d/system-overview"
  
  # High disk usage
  - alert: HighDiskUsage
    expr: instance:node_filesystem_utilization:percent > 85
    for: 10m
    labels:
      severity: warning
      category: system
    annotations:
      summary: "High disk usage on {{ $labels.instance }} ({{ $labels.mountpoint }})"
      description: "Disk usage is {{ $value | printf \"%.2f\" }}% on {{ $labels.mountpoint }} for the last 10 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#high-disk-usage"
      dashboard: "https://grafana.giggatek.com/d/system-overview"
  
  # Critical disk usage
  - alert: CriticalDiskUsage
    expr: instance:node_filesystem_utilization:percent > 95
    for: 5m
    labels:
      severity: critical
      category: system
    annotations:
      summary: "Critical disk usage on {{ $labels.instance }} ({{ $labels.mountpoint }})"
      description: "Disk usage is {{ $value | printf \"%.2f\" }}% on {{ $labels.mountpoint }} for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#critical-disk-usage"
      dashboard: "https://grafana.giggatek.com/d/system-overview"
  
  # Disk will fill in 4 hours
  - alert: DiskFillingSoon
    expr: predict_linear(node_filesystem_avail_bytes{fstype=~"ext4|xfs"}[6h], 4 * 3600) < 0
    for: 30m
    labels:
      severity: warning
      category: system
    annotations:
      summary: "Disk will fill in 4 hours on {{ $labels.instance }} ({{ $labels.mountpoint }})"
      description: "Disk will fill in approximately 4 hours based on the growth rate of the last 6 hours"
      runbook: "docs/monitoring/RUNBOOKS.md#disk-filling-soon"
      dashboard: "https://grafana.giggatek.com/d/system-overview"
  
  # Instance down
  - alert: InstanceDown
    expr: up == 0
    for: 1m
    labels:
      severity: critical
      category: system
    annotations:
      summary: "Instance {{ $labels.instance }} is down"
      description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute"
      runbook: "docs/monitoring/RUNBOOKS.md#instance-down"
      dashboard: "https://grafana.giggatek.com/d/system-overview"

# Database and caching alerts
- name: database_alerts
  rules:
  # MySQL high connection usage
  - alert: MySQLHighConnectionUsage
    expr: mysql:connection_utilization:percent > 80
    for: 5m
    labels:
      severity: warning
      category: database
    annotations:
      summary: "MySQL high connection usage"
      description: "MySQL connection pool is {{ $value | printf \"%.2f\" }}% full for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#mysql-high-connection-usage"
      dashboard: "https://grafana.giggatek.com/d/database-overview"
  
  # MySQL too many slow queries
  - alert: MySQLTooManySlowQueries
    expr: rate(mysql_global_status_slow_queries[5m]) > 5
    for: 10m
    labels:
      severity: warning
      category: database
    annotations:
      summary: "MySQL too many slow queries"
      description: "MySQL has {{ $value | printf \"%.2f\" }} slow queries per second for the last 10 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#mysql-slow-queries"
      dashboard: "https://grafana.giggatek.com/d/database-overview"
  
  # Redis high memory usage
  - alert: RedisHighMemoryUsage
    expr: redis:memory_utilization:percent > 80
    for: 5m
    labels:
      severity: warning
      category: cache
    annotations:
      summary: "Redis high memory usage"
      description: "Redis memory usage is {{ $value | printf \"%.2f\" }}% for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#redis-high-memory-usage"
      dashboard: "https://grafana.giggatek.com/d/cache-overview"
  
  # Redis low hit rate
  - alert: RedisLowHitRate
    expr: redis:keyspace_hit_rate:percent < 70
    for: 15m
    labels:
      severity: warning
      category: cache
    annotations:
      summary: "Redis low hit rate"
      description: "Redis hit rate is {{ $value | printf \"%.2f\" }}% for the last 15 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#redis-low-hit-rate"
      dashboard: "https://grafana.giggatek.com/d/cache-overview"

# Service availability and performance alerts
- name: service_alerts
  rules:
  # API high error rate
  - alert: APIHighErrorRate
    expr: job:http_request_errors:percent{job=~".*api.*"} > 5
    for: 5m
    labels:
      severity: warning
      category: application
    annotations:
      summary: "API has high error rate"
      description: "API error rate is {{ $value | printf \"%.2f\" }}% for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#api-high-error-rate"
      dashboard: "https://grafana.giggatek.com/d/api-overview"
  
  # API critical error rate
  - alert: APICriticalErrorRate
    expr: job:http_request_errors:percent{job=~".*api.*"} > 15
    for: 5m
    labels:
      severity: critical
      category: application
    annotations:
      summary: "API has critical error rate"
      description: "API error rate is {{ $value | printf \"%.2f\" }}% for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#api-critical-error-rate"
      dashboard: "https://grafana.giggatek.com/d/api-overview"
  
  # API endpoint latency
  - alert: APIHighLatency
    expr: job_handler:http_request_duration:p95{job=~".*api.*"} > 1
    for: 5m
    labels:
      severity: warning
      category: application
    annotations:
      summary: "API high latency"
      description: "API 95th percentile latency is {{ $value | printf \"%.2f\" }}s for the last 5 minutes on {{ $labels.handler }}"
      runbook: "docs/monitoring/RUNBOOKS.md#api-high-latency"
      dashboard: "https://grafana.giggatek.com/d/api-overview"
  
  # Website availability
  - alert: WebsiteDown
    expr: probe_success{job="blackbox",instance=~"https://www.giggatek.com"} == 0
    for: 2m
    labels:
      severity: critical
      category: application
    annotations:
      summary: "Website is down"
      description: "The website is not responding to HTTP requests for 2 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#website-down"
      dashboard: "https://grafana.giggatek.com/d/website-overview"

# Order processing alerts
- name: order_alerts
  rules:
  # Order service high latency
  - alert: OrderServiceHighLatency
    expr: job_handler:http_request_duration:p95{job="orders-service"} > 2
    for: 5m
    labels:
      severity: warning
      category: order
    annotations:
      summary: "Order service high latency"
      description: "Order service 95th percentile latency is {{ $value | printf \"%.2f\" }}s for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#order-service-high-latency"
      dashboard: "https://grafana.giggatek.com/d/order-overview"
  
  # Order processing failure rate
  - alert: OrderProcessingHighFailureRate
    expr: 100 - orders:success_rate:percent > 5
    for: 10m
    labels:
      severity: warning
      category: order
    annotations:
      summary: "Order processing has high failure rate"
      description: "Order processing failure rate is {{ $value | printf \"%.2f\" }}% for the last 10 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#order-processing-high-failure-rate"
      dashboard: "https://grafana.giggatek.com/d/order-overview"
  
  # Order processing critical failure rate
  - alert: OrderProcessingCriticalFailureRate
    expr: 100 - orders:success_rate:percent > 15
    for: 5m
    labels:
      severity: critical
      category: order
    annotations:
      summary: "Order processing has critical failure rate"
      description: "Order processing failure rate is {{ $value | printf \"%.2f\" }}% for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#order-processing-critical-failure-rate"
      dashboard: "https://grafana.giggatek.com/d/order-overview"
  
  # Order processing time too high
  - alert: OrderProcessingTimeTooHigh
    expr: orders:processing_time:avg1h > 25
    for: 10m
    labels:
      severity: warning
      category: order
    annotations:
      summary: "Order processing time too high"
      description: "Average order processing time is {{ $value | printf \"%.2f\" }}s for the last hour"
      runbook: "docs/monitoring/RUNBOOKS.md#order-processing-time-too-high"
      dashboard: "https://grafana.giggatek.com/d/order-overview"

# Rental processing alerts
- name: rental_alerts
  rules:
  # Rental service high latency
  - alert: RentalServiceHighLatency
    expr: job_handler:http_request_duration:p95{job="rentals-service"} > 2
    for: 5m
    labels:
      severity: warning
      category: rental
    annotations:
      summary: "Rental service high latency"
      description: "Rental service 95th percentile latency is {{ $value | printf \"%.2f\" }}s for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#rental-service-high-latency"
      dashboard: "https://grafana.giggatek.com/d/rental-overview"
  
  # Rental approval high failure rate
  - alert: RentalApprovalHighFailureRate
    expr: 100 - rentals:application_success:percent > 15
    for: 15m
    labels:
      severity: warning
      category: rental
    annotations:
      summary: "Rental approval has high rejection rate"
      description: "Rental application rejection rate is {{ $value | printf \"%.2f\" }}% for the last 15 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#rental-approval-high-failure-rate"
      dashboard: "https://grafana.giggatek.com/d/rental-overview"
  
  # Rental approval time too high
  - alert: RentalApprovalTimeTooHigh
    expr: rentals:approval_time:avg1d > 20
    for: 1h
    labels:
      severity: warning
      category: rental
    annotations:
      summary: "Rental approval time too high"
      description: "Average rental approval time is {{ $value | printf \"%.2f\" }} hours for the last day"
      runbook: "docs/monitoring/RUNBOOKS.md#rental-approval-time-too-high"
      dashboard: "https://grafana.giggatek.com/d/rental-overview"

# Payment processing alerts
- name: payment_alerts
  rules:
  # Payment service high latency
  - alert: PaymentServiceHighLatency
    expr: job_handler:http_request_duration:p95{job=~".*payment.*"} > 2
    for: 5m
    labels:
      severity: warning
      category: payment
    annotations:
      summary: "Payment service high latency"
      description: "Payment service 95th percentile latency is {{ $value | printf \"%.2f\" }}s for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#payment-service-high-latency"
      dashboard: "https://grafana.giggatek.com/d/payment-overview"
  
  # Payment processing high failure rate
  - alert: PaymentProcessingHighFailureRate
    expr: 100 - payments:success_rate:percent > 5
    for: 10m
    labels:
      severity: warning
      category: payment
    annotations:
      summary: "Payment processing has high failure rate"
      description: "Payment processing failure rate is {{ $value | printf \"%.2f\" }}% for the last 10 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#payment-processing-high-failure-rate"
      dashboard: "https://grafana.giggatek.com/d/payment-overview"
  
  # Payment processing critical failure rate
  - alert: PaymentProcessingCriticalFailureRate
    expr: 100 - payments:success_rate:percent > 10
    for: 5m
    labels:
      severity: critical
      category: payment
    annotations:
      summary: "Payment processing has critical failure rate"
      description: "Payment processing failure rate is {{ $value | printf \"%.2f\" }}% for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#payment-processing-critical-failure-rate"
      dashboard: "https://grafana.giggatek.com/d/payment-overview"

# Authentication alerts
- name: auth_alerts
  rules:
  # Auth service high latency
  - alert: AuthServiceHighLatency
    expr: job_handler:http_request_duration:p95{job="auth-service"} > 1
    for: 5m
    labels:
      severity: warning
      category: auth
    annotations:
      summary: "Auth service high latency"
      description: "Auth service 95th percentile latency is {{ $value | printf \"%.2f\" }}s for the last 5 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#auth-service-high-latency"
      dashboard: "https://grafana.giggatek.com/d/auth-overview"
  
  # High login failure rate
  - alert: HighLoginFailureRate
    expr: rate(login_failure_total[5m]) / rate(login_attempt_total[5m]) > 0.3
    for: 10m
    labels:
      severity: warning
      category: auth
    annotations:
      summary: "High login failure rate"
      description: "Login failure rate is {{ $value | printf \"%.2f\" }}% for the last 10 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#high-login-failure-rate"
      dashboard: "https://grafana.giggatek.com/d/auth-overview"
  
  # Unusual number of password resets
  - alert: UnusualPasswordResetActivity
    expr: rate(password_reset_request_total[10m]) > 5 * avg_over_time(rate(password_reset_request_total[10m])[1d:10m])
    for: 10m
    labels:
      severity: warning
      category: auth
    annotations:
      summary: "Unusual number of password resets"
      description: "Password reset activity is {{ $value | printf \"%.2f\" }}x higher than normal"
      runbook: "docs/monitoring/RUNBOOKS.md#unusual-password-reset-activity"
      dashboard: "https://grafana.giggatek.com/d/auth-overview"

# SLA alerts
- name: sla_alerts
  rules:
  # Website availability SLA breach
  - alert: WebsiteAvailabilitySLABreach
    expr: sla:website_availability:ratio_30d < 0.999
    for: 5m
    labels:
      severity: warning
      category: sla
    annotations:
      summary: "Website availability SLA breach"
      description: "Website availability is {{ $value | humanizePercentage }} over 30 days, below 99.9% SLA target"
      runbook: "docs/monitoring/RUNBOOKS.md#website-availability-sla-breach"
      dashboard: "https://grafana.giggatek.com/d/sla-overview"
  
  # API availability SLA breach
  - alert: APIAvailabilitySLABreach
    expr: sla:api_availability:ratio_30d < 0.9995
    for: 5m
    labels:
      severity: warning
      category: sla
    annotations:
      summary: "API availability SLA breach"
      description: "API availability is {{ $value | humanizePercentage }} over 30 days, below 99.95% SLA target"
      runbook: "docs/monitoring/RUNBOOKS.md#api-availability-sla-breach"
      dashboard: "https://grafana.giggatek.com/d/sla-overview"
  
  # Order processing time SLA breach
  - alert: OrderProcessingSLABreach
    expr: sla:order_processing:under_30s_ratio < 0.95
    for: 15m
    labels:
      severity: warning
      category: sla
    annotations:
      summary: "Order processing SLA breach"
      description: "Only {{ $value | humanizePercentage }} of orders processed within 30s, below 95% SLA target"
      runbook: "docs/monitoring/RUNBOOKS.md#order-processing-sla-breach"
      dashboard: "https://grafana.giggatek.com/d/sla-overview"
  
  # Transaction success SLA breach
  - alert: TransactionProcessingSLABreach
    expr: sla:transaction_success:ratio < 0.998
    for: 10m
    labels:
      severity: warning
      category: sla
    annotations:
      summary: "Transaction processing SLA breach"
      description: "Transaction success rate is {{ $value | humanizePercentage }}, below 99.8% SLA target"
      runbook: "docs/monitoring/RUNBOOKS.md#transaction-processing-sla-breach"
      dashboard: "https://grafana.giggatek.com/d/sla-overview"
  
  # API response time SLA breach
  - alert: APIResponseTimeSLABreach
    expr: sla:api_response:under_1s_ratio < 0.95
    for: 15m
    labels:
      severity: warning
      category: sla
    annotations:
      summary: "API response time SLA breach"
      description: "Only {{ $value | humanizePercentage }} of API responses under 1s, below 95% SLA target"
      runbook: "docs/monitoring/RUNBOOKS.md#api-response-time-sla-breach"
      dashboard: "https://grafana.giggatek.com/d/sla-overview"

# Business metrics alerts
- name: business_alerts
  rules:
  # Significant order volume drop
  - alert: OrderVolumeSignificantDrop
    expr: (orders:processed:rate1h / avg_over_time(orders:processed:rate1h[24h] offset 1d)) < 0.7
    for: 1h
    labels:
      severity: warning
      category: business
    annotations:
      summary: "Significant drop in order volume"
      description: "Order volume is {{ $value | printf \"%.2f\" }}x compared to same time yesterday"
      runbook: "docs/monitoring/RUNBOOKS.md#order-volume-significant-drop"
      dashboard: "https://grafana.giggatek.com/d/business-overview"
  
  # High cart abandonment rate
  - alert: HighCartAbandonmentRate
    expr: rate(shopping_cart_abandoned_total[30m]) / rate(shopping_cart_created_total[30m]) > 0.8
    for: 30m
    labels:
      severity: warning
      category: business
    annotations:
      summary: "High cart abandonment rate"
      description: "Cart abandonment rate is {{ $value | humanizePercentage }} in the last 30 minutes"
      runbook: "docs/monitoring/RUNBOOKS.md#high-cart-abandonment-rate"
      dashboard: "https://grafana.giggatek.com/d/business-overview"
  
  # Unusual rental application volume
  - alert: UnusualRentalApplicationVolume
    expr: abs(rate(rental_application_total[1h]) / avg_over_time(rate(rental_application_total[1h])[7d:1h]) - 1) > 0.5
    for: 2h
    labels:
      severity: warning
      category: business
    annotations:
      summary: "Unusual rental application volume"
      description: "Rental application volume is {{ $value | printf \"%.2f\" }}x compared to historical average"
      runbook: "docs/monitoring/RUNBOOKS.md#unusual-rental-application-volume"
      dashboard: "https://grafana.giggatek.com/d/business-overview"
